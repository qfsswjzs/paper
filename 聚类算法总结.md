# 常见聚类算法
## 1. KMeans
### 1.1. KMeans
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
       KMeans算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。

​      如果用数据表达式表示，假设簇划分为$(𝐶_1,𝐶_2,...𝐶_k)$，则我们的目标是最小化平方误差E:
$$
E=\sum_{i=1}^k\sum_{x\in C_{i}}\left \|x-\mu_i \right \|_2^2
$$

其中𝜇𝑖是簇𝐶𝑖的均值向量，有时也称为质心，表达式为:   


$$
\mu_i=\frac{1}{|C_i|}\sum_{x\in C_i}x
$$
**算法流程**
输入是样本集𝐷={$𝑥_1,𝑥_2,...𝑥_m$},聚类的簇数k,最大迭代次数N;输出是簇划分𝐶=${𝐶_1,𝐶_2,...𝐶_k}$
1.从数据集D中随机选择k个样本作为初始的k个质心向量：${\mu_1,\mu_2,...,\mu_k}$
2.对于n=1,2,...,N
　a.将簇划分C初始化为$C_t=\emptyset$ ,𝑡=1,2...𝑘 
　b.对于i=1,2...m,计算样本$x_i$和各个质心向量
$\mu_j$(𝑗=1,2,...𝑘)的距离：$d_{ij}=\left \| x_i-\mu_j|\right |_2^2$，将$x_i$标记最小的为$d_{ij}$所对应的类别$\lambda_i$。此时更新$C_{\lambda_i}=C_{\lambda_i}U{x_i}$</br>
　c.对于j=1,2,...,k,对$𝐶_j$中所有的样本点重新计算新的质心$\mu_j=\frac{1}{|C_j|}\sum_{x\in C_j}x$</br>
  d.如果所有的k个质心向量都没有发生变化，则转到步骤3</br>
3.输出簇划分𝐶=$\{𝐶_1,𝐶_2,...𝐶_k\}$</br>   
**算法缺点**</br>
1.K值的选取不好把握</br>
2.对于不是凸的数据集比较难收敛</br>
3.如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。</br>
4.采用迭代方法，得到的结果只是局部最优。</br>
5.对噪音和异常点敏感。</br>   

### 1.2. Mini Batch KMeans
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
传统的KMeans算法中，要计算所有的样本点到所有的质心的距离。如果样本量非常大，用传统的K-Means算法非常的耗时。在此时，Mini Batch KMeans应运而生。顾名思义，Mini Batch，也就是用样本集中的一部分的样本来做传统的KMeans，这样可以避免样本量太大时的计算难题，算法收敛速度大大加快。当然此时的代价就是我们的聚类的精确度也会有一些降低，一般来说这个降低的幅度在可以接受的范围之内。  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
在Mini Batch KMeans中，我们会选择一个合适的批样本大小batch size，我们仅仅用batch size个样本来做KMeans聚类。这batch size个样本一般是通过无放回的随机采样得到的。为了增加算法的准确性，一般会多跑几次Mini Batch K-Means算法，用得到不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。   
## 2.AffinityPropagation
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
AP算法的基本思想：将全部样本看作网络的节点，然后通过网络中各条边的消息传递计算出各样本的聚类中心。聚类过程中，共有两种消息在各节点间传递，分别是吸引度(responsibility)和归属度(availability) 。AP算法通过迭代过程不断更新每一个点的吸引度和归属度值，直到产生m个高质量的Exemplar（类似于质心），同时将其余的数据点分配到相应的聚类中。<br>   
**算法流程**<br>
1.算法初始，将吸引度矩阵R和归属度矩阵初始化为0矩阵</br>
2.更新吸引度矩阵
$$r(i,k)=\begin{cases}
    s(i,k)-max_{j\neq k}\{a(i,j)+s(i,j)\},&i\neq k,\\
    s(i,k)-max_{j\neq k}\{s(i,j)\},&i=k.
\end{cases}$$
3.更新归属度矩阵
$$a(i,k)=\begin{cases}
    min\{{0,r(k,k)+\sum_{j\neq i,k}max\{r(j,k),0\}}\},&i\neq k\\
    \sum_{j\neq k}max\{r(j,k),0\},&i=k
\end{cases}$$
4.在公式中使用阻尼系数$\lambda$
$$r_{t+1}(i,k)=\lambda\cdot r_t(i,k)+(1-\lambda)\cdot r_{t+1}(i,k)$$
$$a_{t+1}(i,k)=\lambda\cdot a_t(i,k)+(1-\lambda)\cdot a_{t+1}(i,k)$$
阻尼系数$\lambda$用于防止结果振荡</br>   
**算法特点**</br>
1.初始值不敏感。多次执行AP聚类算法，得到的结果是完全一样的，即不需要进行随机选取初值步骤。</br>
2.算法复杂度较高，为$O(N^2T)$，N为样本数，T为迭代次数。</br>
3.AP算法相对K-Means鲁棒性强且准确度较高。</br>   
4.特别适合高维、多类数据快速聚类，相比传统的聚类算法，该算法算是比较新的，从聚类性能和效率方面都有大幅度的提升。

## 3.Mean Shift 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
对于给定的𝑑维空间$R_d$中的n个样本点$x_i$, 𝑖=1,⋯,𝑛，则对于x点，其Mean Shift向量的基本形式为：
$$M_h(x)=\frac{1}{k}\sum_{x_i\in S_h}(x_i-x)$$
![msv](msv.png)   
其中$S_h$指的是一个半径为h的高维球区域，如上图中的圆形区域。$S_h$的定义为：
$$
S_h(x)=\{y|(y-x)(y-x)^T\leq h^2\}
$$
对于Mean Shift算法，是一个迭代的步骤，即先算出当前点的偏移均值，将该点移动到此偏移均值，然后以此为新的起始点，继续移动，直到满足最终的条件。
![msvd](msv.gif)   
如上的均值漂移向量的求解方法存在一个问题，即在$S_h$的区域内，每一个样本点x对样本X的贡献是一样的。而实际中，每一个样本点x对样本X的贡献是不一样的，这样的共享可以通过核函数进行度量。</br>   
**引入核函数的Mean Shift向量**</br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
假设在半径为h的范围$S_h$范围内，为了使得每一个样本点x对于样本X的贡献不一样，向基本的Mean Shift向量形式中增加核函数，得到如下改进的Mean Shift向量形式：
$$
M_h(x)=\frac{\sum_{i=1}^n G(\frac{x_i-x}{h_i})w(x_i)(x_i-x)}{\sum_{i=1}^n G(\frac{x_i-x}{h_i})w(x_i)}
$$
![msvh](msh.gif)   
**算法流程**</br>
1.计算每个样本的mean shift向量$m_h(x)$</br>
2.对每个样本点以$m_h(x)$进行平移，即：
$$
x_i=x_i+m_h(x_i)
$$
3.重复1,2直到样本点收敛</br>
4.收敛到相同点的样本被认为是同一簇类的成员</br>   
**优缺点**</br>
1.不需要设置簇类的个数</br>
2.可以处理任意形状的簇类</br>
3.算法只需设置带宽这一个参数，带宽影响数据集的核密度估计</br>
4.聚类结果取决于带宽的设置，带宽设置的太小，收敛太慢，簇类个数过多；带宽设置的太大，一些簇类可能会丢失</br>
5.对于较大的特征空间，计算量非常大</br>   

## 4.SpectralClustering

**算法流程**

输入：样本集D=$(x_1,x_2,...,x_n)$, 降维后的维度$k1$, 聚类方法，聚类后的维度$k2$

输出：簇划分$C(c_1,c_2,...,c_n)$.

1.根据输入的相似矩阵的生成方式构建样本的相似矩阵S

2.根据相似矩阵S构建邻接矩阵W，构建度矩阵D

3.计算出拉普拉斯矩阵L

4.构建标准化后的拉普拉斯矩阵$D^{-1/2}LD^{-1/2}$

5.计算$D^{-1/2}LD^{-1/2}$最小的$k1$个特征值所各自对应的特征向量$f$

6.将各自对应的特征向量$f$组成的矩阵按行标准化，最终组成$n×k1$维的特征矩阵F

7.对F中的每一行作为一个$k1$维的样本，共n个样本，用输入的聚类方法进行聚类，聚类维数为$k2$。

8.得到簇划分$C(c_1,c_2,...,c_n)$.

**优缺点**

1.谱聚类只需要数据之间的相似度矩阵，因此对于处理稀疏数据的聚类很有效。这点传统聚类算法比如K-Means很难做到

2.由于使用了降维，因此在处理高维数据聚类时的复杂度比传统聚类算法好

3.如果最终聚类的维度非常高，则由于降维的幅度不够，谱聚类的运行速度和最后的聚类效果均不好

4.聚类效果依赖于相似矩阵，不同的相似矩阵得到的最终聚类效果可能很不同



